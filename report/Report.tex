\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{subfig}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%
\usepackage[alpha]{mdpn}
\usepackage{tikz}
\usetikzlibrary{positioning,angles,quotes}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{program}
\tikzset{%
  every neuron/.style={
    circle,
    draw,
    minimum size=1cm
  },
  neuron missing/.style={
    draw=none, 
    scale=4,
    text height=0.333cm,
    execute at begin node=\color{black}$\vdots$
  },
}
%
\begin{document}
%
\title{Solving Markov Decision Processes using Deep Q-learning}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Andrey Antonowycz}
%
\authorrunning{Andrey Antonowycz}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{University of Twente}
%
\maketitle % typeset the header of the contribution
%
\begin{abstract}
Q-learning is a model-free machine learning algorithm used to determine the optimal action for a given input state. This is achieved by navigating through a model and recording rewards when crossing a transient between two states. The action taken, along with the state it was taken in is recorded and the reward along with possible rewards in the new post-transient state are recorded in an ever-expanding table. The method of optimizing using Q-learning for a given property has proven useful in existing tools \cite{modest} for solving properties in Markov Decision Processes (MDPs). Deep Q-learning is similar in its method of recording, but replaces the process of querying a Q-table with querying a neural network. However, together with other Reinforcement Learning (RL) techniques, it is most often used in domains other than model checking. E.g., finding the optimal action to take given a set of environmental factors and a constant action space. In this paper we are interested in the applicability of using a Deep Q neural network to solve properties in a PRISM model. We will compare different approaches in encoding the action space, and evaluate the accuracy of the resulting model. Multiple methods of encoding the action space are evaluated, including a one-hot encoding and an ordinal encoding. Furthermore, when encoding the action space as either one-hot or ordinal, either a Q-table approach and an input layer approach is utilized. The effects of Double Q-learning as opposed to regular Q-learning are compared, where in Double Q-learning the target and policy network are separated.

\keywords{Model checking \and MDP \and Deep Q-learning.}
\end{abstract}
%
%
%
\newpage
\input{sections/Introduction}
\input{sections/Encoding the state-space}
\input{sections/Encoding the action-space}
\input{sections/Methodology}
\input{sections/Results}
\input{sections/Conclusion}
\input{sections/Discussion}

\newpage
\subsubsection{Acknowledgements}

In the process of writing this paper, I have had the pleasure of working with dr. Arnd Hartmanns and dr. Moritz Hahn at the University of Twente. I would like to thank them for their guidance and support during the development and writing process. Our brainstorming sessions have been invaluable in the process of developing the ideas presented in this paper.

The model checker using Deep Q-learning is based off a regular Q-learning solver built by Alex Jauregui Morris and I, and uses a Python representation of Modest and Jani models parsed and exported using the Modest Toolset \cite{modest}.


%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{bibliography}

\end{document}
